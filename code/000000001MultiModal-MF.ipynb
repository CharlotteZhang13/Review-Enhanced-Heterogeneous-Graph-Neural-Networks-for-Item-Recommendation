{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389410e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users_s 5130\n",
      "num_items_s 1685\n",
      "userNum_s 5130\n",
      "itemNum_s 1685\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#以下为数据预处理部分####data_preprocess\n",
    "###################################################################################################\n",
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "df_s = pd.read_csv(\"E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/Amazon_Instant_Video_noreview.csv\", sep=',')\n",
    "file_path_s = 'E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/Amazon_Instant_Video_noreview.csv'\n",
    "path_s = '/'.join(file_path_s.split('/')[:-1])\n",
    "name_s = file_path_s.split('/')[-1].split('_')[0]\n",
    "train_npy_path_s = file_path_s.replace('.csv', '_train.npy')\n",
    "test_npy_path_s = file_path_s.replace('.csv', '_test.npy')\n",
    "def _load_data(df):\n",
    "    uuu=0\n",
    "    iii=0\n",
    "    for user, item in zip(df['uid'], df['iid']):\n",
    "        if uuu < user:\n",
    "            uuu = user\n",
    "        if iii < item:\n",
    "            iii = item        \n",
    "    return uuu+1,iii+1\n",
    "num_users_s, num_items_s = _load_data(df_s)\n",
    "print(\"num_users_s\",num_users_s)\n",
    "print(\"num_items_s\",num_items_s)\n",
    "\n",
    "def _construct_pos_dict(df):\n",
    "    pos_dict = defaultdict(set)\n",
    "    for user, item in zip(df['uid'], df['iid']):\n",
    "        pos_dict[user].add(item)\n",
    "    return pos_dict\n",
    "pos_dict_s=_construct_pos_dict(df_s)\n",
    "\n",
    "def read_data(dataPath):\n",
    "    inFile = open(dataPath + 'data_HGNR.pkl','rb')\n",
    "    data = pickle.load(inFile)\n",
    "    userNum, itemNum = data['userNum'], data['itemNum']\n",
    "    trainData, testData, testNegData  = data['trainData'], data['testData'], data['testNegData']\n",
    "    inFile.close()\n",
    "    return trainData, testData, testNegData,userNum, itemNum\n",
    "dataPath1 = \"E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/\"\n",
    "trainData_s, testData_s,testNegData_s, userNum_s, itemNum_s = read_data(dataPath1)\n",
    "print('userNum_s',userNum_s)\n",
    "print('itemNum_s',itemNum_s)\n",
    "train_df_s1,train_df_s2 = [],[]\n",
    "test_df_s1,test_df_s2 = [],[]\n",
    "train_df_s,test_df_s = {'uid':[],'iid':[]},{'uid':[],'iid':[]}\n",
    "train_df_s,test_df_s =  pd.DataFrame(train_df_s),pd.DataFrame(test_df_s)\n",
    "for u,i in trainData_s:   \n",
    "    train_df_s1.append(u)\n",
    "    train_df_s2.append(i)\n",
    "train_df_s['uid'] = train_df_s1\n",
    "train_df_s['iid'] = train_df_s2 \n",
    "train_df_s.to_csv(path_s+'/%s_train_df.csv'% name_s, index=False)\n",
    "for u,i in testData_s:\n",
    "    test_df_s1.append(u)\n",
    "    test_df_s2.append(i)\n",
    "test_df_s['uid'] = test_df_s1\n",
    "test_df_s['iid'] = test_df_s2 \n",
    "test_df_s.to_csv(path_s+'/%s_test_df.csv'% name_s, index=False)\n",
    "\n",
    "def _add_negtive(user, item, num_items, pos_dict, neg_num, boolindex):\n",
    "    user, item, num_items, pos_dict, neg_num, train = user, item, num_items, pos_dict,neg_num ,boolindex\n",
    "    users, items, labels = [], [], []\n",
    "    neg_set = set(range(num_items)).difference(pos_dict[user])  #difference用于返回集合的差集，包含在第一个集合中，但不包含在第二个集合中#\n",
    "    neg_sample_list = np.random.choice(list(neg_set), neg_num, replace=False).tolist()\n",
    "    for neg_sample in neg_sample_list:\n",
    "        users.append(user)\n",
    "        items.append(neg_sample)\n",
    "        labels.append(0) if train == True else labels.append(neg_sample)\n",
    "    users.append(user)\n",
    "    items.append(item)\n",
    "    if train == True:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(int(item))\n",
    "    return (users, items, labels)\n",
    "\n",
    "users = []\n",
    "items = []\n",
    "labels = []    \n",
    "for user, item in zip(train_df_s['uid'], train_df_s['iid']):\n",
    "    batch_users, batch_items, batch_labels = _add_negtive(user, item, num_items_s, pos_dict_s,4 ,True)\n",
    "    users += batch_users\n",
    "    items += batch_items\n",
    "    labels += batch_labels\n",
    "\"\"\"\n",
    "str --> int\n",
    "results = ['1','2','3']\n",
    "results = list(map(int, results))    \n",
    "\"\"\"\n",
    "users = list(map(int, users))\n",
    "items = list(map(int, items))\n",
    "labels = list(map(int, labels))\n",
    "data_dict_str = {'user': users, 'item': items, 'label': labels}\n",
    "np.save(train_npy_path_s, data_dict_str)\n",
    "users = []\n",
    "items = []\n",
    "labels = []    \n",
    "for user, item in zip(test_df_s['uid'], test_df_s['iid']):\n",
    "    batch_users, batch_items, batch_labels = _add_negtive(user, item, num_items_s, pos_dict_s,99 ,False)\n",
    "    users += batch_users\n",
    "    items += batch_items\n",
    "    labels += batch_labels\n",
    "users = list(map(int, users))\n",
    "items = list(map(int, items))\n",
    "labels = list(map(int, labels))\n",
    "data_dict_ste = {'user': users, 'item': items, 'label': labels}\n",
    "np.save(test_npy_path_s, data_dict_ste)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b32c495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(NCForMF='NCF', batch_size=100, data_rebuild=False, dropout_message=0, embedding_size=64, epochs=150, gpu_deviild=False, lr=0.005, mat_rebuild=False, mlp_layers=[32], regularizer_rate=8e-05, test_neg_num=99, test_size=1, topK=10, train_neg_num=4)\n"
     ]
    }
   ],
   "source": [
    "#以下为模型部分####Model\n",
    "###################################################################################################\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser的主要作用就是把字符串的代码ce', type=int, default=0,\n",
    "\n",
    "\n",
    "parser.add_argument('--gpu_deviild', type=bool, default=False,\n",
    "                    help='whether to rebuild cross data')\n",
    "parser.add_argument('--data_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild train/test dataset')\n",
    "parser.add_argument('--mat_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild` adjacent mat')\n",
    "parser.add_argument('--batch_size', type=int, default=100,\n",
    "                    help='size of mini-batch')\n",
    "parser.add_argument('--train_neg_num', type=int, default=4,\n",
    "                    help='number of negative samples per training positive sample')\n",
    "parser.add_argument('--test_size', type=int, default=1,\n",
    "                    help='size of sampled test data')\n",
    "parser.add_argument('--test_neg_num', type=int, default=99,\n",
    "                    help='number of negative samples for test')\n",
    "parser.add_argument('--epochs', type=int, default=150,\n",
    "                    help='the number of epochs')\n",
    "parser.add_argument('--mlp_layers', nargs='?', default=[32],\n",
    "                    help='the unit list of layers')\n",
    "parser.add_argument('--embedding_size', type=int, default=64,\n",
    "                    help='the size for embedding user and item')\n",
    "parser.add_argument('--topK', type=int, default=10,\n",
    "                    help='topk for evaluation')\n",
    "parser.add_argument('--regularizer_rate', type=float, default=8e-5,   ####default=0.01\n",
    "                    help='the regularizer rate')\n",
    "parser.add_argument('--lr', type=float, default=0.005,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--dropout_message', type=float, default=0,  ####default=0.1\n",
    "                    help='dropout rate of message')\n",
    "parser.add_argument('--NCForMF', type=str, default='NCF',\n",
    "                    help='method to propagate embeddings')\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "import os, sys, time\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "\n",
    "class MF_model(object):\n",
    "    def __init__(self, args, iterator, num_users, num_items_s,is_training):\n",
    "        self.args = args\n",
    "        self.iterator = iterator\n",
    "        self.num_users = num_users\n",
    "        self.num_items_s = num_items_s\n",
    "        self.is_training = is_training\n",
    "        self.n_fold = 50\n",
    "        self.get_data()\n",
    "        self.all_weights = self.init_weights()       \n",
    "        self.item_embeddings_s1, self.user_embeddings1 = self.all_weights['item_embeddings_s1'], self.all_weights['user_embeddings1']\n",
    "        self.item_embeddings_s = self.item_embeddings_s1 \n",
    "        self.user_embeddings = self.user_embeddings1 \n",
    "        self.inference()\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "    def get_data(self):#导入数据\n",
    "        sample = self.iterator.get_next()\n",
    "        self.user, self.item_s= sample['user'], sample['item']\n",
    "        self.label_s = tf.cast(sample['label'], tf.float32)\n",
    "    def init_weights(self):#随机初始化所有参数\n",
    "        all_weights = dict()\n",
    "        initializer = tf.truncated_normal_initializer(0.01)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.args.regularizer_rate)\n",
    "        all_weights['user_embeddings1'] = tf.get_variable(\n",
    "            'user_embeddings1', (self.num_users, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        #一个用户就对应设置embedding大小个参数，随机初始化\n",
    "        #（因为反正独热都是只有一个1其他全是0，因此这个embedding就是可以代表任意一个客户或者商品）\n",
    "        all_weights['item_embeddings_s1'] = tf.get_variable(\n",
    "            'item_embeddings_s1', (self.num_items_s, self.args.embedding_size), tf.float32, initializer, regularizer)     \n",
    "        return all_weights\n",
    "\n",
    "    def inference(self):      \n",
    "        initializer = tf.truncated_normal_initializer(0.01)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.args.regularizer_rate)\n",
    "        with tf.name_scope('embedding'):\n",
    "            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user)#打乱的user编号找到对应的embedding\n",
    "            item_embedding_s = tf.nn.embedding_lookup(self.item_embeddings_s, self.item_s)\n",
    "        with tf.name_scope('propagation'):\n",
    "            if self.args.NCForMF == 'MF':#经典简单的matrix factorization\n",
    "                self.logits_dense_s = tf.reduce_sum(tf.multiply(user_embedding, item_embedding_s), 1)\n",
    "                #user_embedding和item_embedding_s相乘得到内积  \n",
    "            elif self.args.NCForMF == 'NCF':\n",
    "                a_s = tf.concat([user_embedding, item_embedding_s], axis=-1, name='inputs_s')\n",
    "                #把用户和商品学习到的embedding给连接起来\n",
    "                for i, units in enumerate(self.args.mlp_layers):\n",
    "                    dense_s = tf.layers.dense(a_s, units, tf.nn.relu, kernel_initializer=initializer,\n",
    "                                          kernel_regularizer = regularizer, name='dense_s_%d' % i)\n",
    "                    a_s = tf.layers.dropout(dense_s, self.args.dropout_message)\n",
    "                    #搭建三层全连接+dropout层，给用户和商品的embedding的交互关系做非线性变换\n",
    "                self.logits_dense_s = tf.layers.dense(inputs=a_s,\n",
    "                                                      units=1,\n",
    "                                                      kernel_initializer=initializer,\n",
    "                                                      kernel_regularizer=regularizer,\n",
    "                                                      name='logits_dense_s')\n",
    "            else:\n",
    "                raise ValueError\n",
    "            self.logits_s = tf.squeeze(self.logits_dense_s)\n",
    "\n",
    "            loss_list_s = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_s, logits=self.logits_s,name='loss_s')\n",
    "            loss_w_s = tf.map_fn(lambda x: tf.cond(tf.equal(x, 1.0), lambda: 5.0, lambda: 1.0), self.label_s)\n",
    "            #这个其实就是把标签是1的损失权重设置大一点，因为还是预测更多对的重要一些\n",
    "            self.loss_s = tf.reduce_mean(tf.multiply(loss_list_s, loss_w_s))\n",
    "            self.loss = self.loss_s #计算损失\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.args.lr).minimize(self.loss)#优化器做梯度下降\n",
    "            self.label_replica_s = self.label_s\n",
    "            _, self.indice_s = tf.nn.top_k(tf.sigmoid(self.logits_s), self.args.topK)\n",
    "    def step(self, sess):\n",
    "        if self.is_training:\n",
    "            label_s, indice_s, loss, optim = sess.run(\n",
    "                [self.label_replica_s, self.indice_s, self.loss, self.optimizer])\n",
    "            return loss\n",
    "        else:\n",
    "            label_s, indice_s = sess.run([self.label_replica_s, self.indice_s])\n",
    "            prediction_s = np.take(label_s, indice_s)\n",
    "            return prediction_s, label_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fab441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data <BatchDataset shapes: {user: (?,), item: (?,), label: (?,)}, types: {user: tf.int32, item: tf.int32, label: tf.int32}>\n",
      "train_data除以batch_size 1599.8\n",
      "test_data <TensorSliceDataset shapes: {user: (), item: (), label: ()}, types: {user: tf.int32, item: tf.int32, label: tf.int32}>\n",
      "Creating model with fresh parameters...\n",
      "============================== EPOCH 1 ==============================\n",
      "Epoch 1, step 300, with average loss of 7.8396 in last 300 steps;\n",
      "Epoch 1, step 600, with average loss of 2.6475 in last 300 steps;\n",
      "Epoch 1, step 900, with average loss of 1.3096 in last 300 steps;\n",
      "Epoch 1, step 1200, with average loss of 1.1803 in last 300 steps;\n",
      "Epoch 1, step 1500, with average loss of 1.1670 in last 300 steps;\n",
      "Epoch 1, finish training took 00: 00: 07;\n",
      "Epoch 1, finish testing took: 00: 00: 04;\n",
      "Epoch 1, Amazon HR is 0.3291, MRR is 0.1335, NDCG is 0.1790;\n",
      "============================== EPOCH 2 ==============================\n",
      "Epoch 2, step 1800, with average loss of 1.0652 in last 300 steps;\n",
      "Epoch 2, step 2100, with average loss of 1.0541 in last 300 steps;\n",
      "Epoch 2, step 2400, with average loss of 1.0250 in last 300 steps;\n",
      "Epoch 2, step 2700, with average loss of 1.0471 in last 300 steps;\n",
      "Epoch 2, step 3000, with average loss of 1.0227 in last 300 steps;\n",
      "Epoch 2, finish training took 00: 00: 08;\n",
      "Epoch 2, finish testing took: 00: 00: 03;\n",
      "Epoch 2, Amazon HR is 0.3794, MRR is 0.1688, NDCG is 0.2182;\n",
      "============================== EPOCH 3 ==============================\n",
      "Epoch 3, step 3300, with average loss of 1.0035 in last 300 steps;\n",
      "Epoch 3, step 3600, with average loss of 0.9455 in last 300 steps;\n",
      "Epoch 3, step 3900, with average loss of 0.9452 in last 300 steps;\n",
      "Epoch 3, step 4200, with average loss of 0.9750 in last 300 steps;\n",
      "Epoch 3, step 4500, with average loss of 0.9864 in last 300 steps;\n",
      "Epoch 3, step 4800, with average loss of 0.9789 in last 300 steps;\n",
      "Epoch 3, finish training took 00: 00: 07;\n",
      "Epoch 3, finish testing took: 00: 00: 02;\n",
      "Epoch 3, Amazon HR is 0.3884, MRR is 0.1692, NDCG is 0.2205;\n",
      "============================== EPOCH 4 ==============================\n",
      "Epoch 4, step 5100, with average loss of 0.8131 in last 300 steps;\n",
      "Epoch 4, step 5400, with average loss of 0.8523 in last 300 steps;\n",
      "Epoch 4, step 5700, with average loss of 0.8958 in last 300 steps;\n",
      "Epoch 4, step 6000, with average loss of 0.9014 in last 300 steps;\n",
      "Epoch 4, step 6300, with average loss of 0.9283 in last 300 steps;\n",
      "Epoch 4, finish training took 00: 00: 06;\n",
      "Epoch 4, finish testing took: 00: 00: 02;\n",
      "Epoch 4, Amazon HR is 0.3880, MRR is 0.1636, NDCG is 0.2159;\n",
      "============================== EPOCH 5 ==============================\n",
      "Epoch 5, step 6600, with average loss of 0.7658 in last 300 steps;\n",
      "Epoch 5, step 6900, with average loss of 0.7108 in last 300 steps;\n",
      "Epoch 5, step 7200, with average loss of 0.7629 in last 300 steps;\n",
      "Epoch 5, step 7500, with average loss of 0.7834 in last 300 steps;\n",
      "Epoch 5, step 7800, with average loss of 0.8320 in last 300 steps;\n",
      "Epoch 5, finish training took 00: 00: 06;\n",
      "Epoch 5, finish testing took: 00: 00: 02;\n",
      "Epoch 5, Amazon HR is 0.3632, MRR is 0.1505, NDCG is 0.2001;\n",
      "============================== EPOCH 6 ==============================\n",
      "Epoch 6, step 8100, with average loss of 0.7386 in last 300 steps;\n",
      "Epoch 6, step 8400, with average loss of 0.5520 in last 300 steps;\n",
      "Epoch 6, step 8700, with average loss of 0.5999 in last 300 steps;\n",
      "Epoch 6, step 9000, with average loss of 0.6606 in last 300 steps;\n",
      "Epoch 6, step 9300, with average loss of 0.6798 in last 300 steps;\n",
      "Epoch 6, step 9600, with average loss of 0.7058 in last 300 steps;\n",
      "Epoch 6, finish training took 00: 00: 07;\n",
      "Epoch 6, finish testing took: 00: 00: 02;\n",
      "Epoch 6, Amazon HR is 0.3453, MRR is 0.1393, NDCG is 0.1874;\n",
      "============================== EPOCH 7 ==============================\n",
      "Epoch 7, step 9900, with average loss of 0.4235 in last 300 steps;\n",
      "Epoch 7, step 10200, with average loss of 0.4494 in last 300 steps;\n",
      "Epoch 7, step 10500, with average loss of 0.5150 in last 300 steps;\n",
      "Epoch 7, step 10800, with average loss of 0.5471 in last 300 steps;\n",
      "Epoch 7, step 11100, with average loss of 0.5855 in last 300 steps;\n",
      "Epoch 7, finish training took 00: 00: 07;\n",
      "Epoch 7, finish testing took: 00: 00: 02;\n",
      "Epoch 7, Amazon HR is 0.3225, MRR is 0.1278, NDCG is 0.1731;\n",
      "============================== EPOCH 8 ==============================\n",
      "Epoch 8, step 11400, with average loss of 0.4115 in last 300 steps;\n",
      "Epoch 8, step 11700, with average loss of 0.3378 in last 300 steps;\n",
      "Epoch 8, step 12000, with average loss of 0.3830 in last 300 steps;\n",
      "Epoch 8, step 12300, with average loss of 0.4330 in last 300 steps;\n",
      "Epoch 8, step 12600, with average loss of 0.4702 in last 300 steps;\n",
      "Epoch 8, finish training took 00: 00: 07;\n",
      "Epoch 8, finish testing took: 00: 00: 02;\n",
      "Epoch 8, Amazon HR is 0.3283, MRR is 0.1232, NDCG is 0.1708;\n",
      "============================== EPOCH 9 ==============================\n",
      "Epoch 9, step 12900, with average loss of 0.4007 in last 300 steps;\n",
      "Epoch 9, step 13200, with average loss of 0.2659 in last 300 steps;\n",
      "Epoch 9, step 13500, with average loss of 0.2927 in last 300 steps;\n",
      "Epoch 9, step 13800, with average loss of 0.3397 in last 300 steps;\n",
      "Epoch 9, step 14100, with average loss of 0.3948 in last 300 steps;\n",
      "Epoch 9, step 14400, with average loss of 0.4002 in last 300 steps;\n",
      "Epoch 9, finish training took 00: 00: 07;\n",
      "Epoch 9, finish testing took: 00: 00: 02;\n",
      "Epoch 9, Amazon HR is 0.3199, MRR is 0.1241, NDCG is 0.1697;\n",
      "============================== EPOCH 10 ==============================\n",
      "Epoch 10, step 14700, with average loss of 0.2206 in last 300 steps;\n",
      "Epoch 10, step 15000, with average loss of 0.2352 in last 300 steps;\n",
      "Epoch 10, step 15300, with average loss of 0.2693 in last 300 steps;\n",
      "Epoch 10, step 15600, with average loss of 0.2994 in last 300 steps;\n",
      "Epoch 10, step 15900, with average loss of 0.3332 in last 300 steps;\n",
      "Epoch 10, finish training took 00: 00: 06;\n",
      "Epoch 10, finish testing took: 00: 00: 02;\n",
      "Epoch 10, Amazon HR is 0.3094, MRR is 0.1165, NDCG is 0.1613;\n",
      "============================== EPOCH 11 ==============================\n",
      "Epoch 11, step 16200, with average loss of 0.2395 in last 300 steps;\n",
      "Epoch 11, step 16500, with average loss of 0.1818 in last 300 steps;\n",
      "Epoch 11, step 16800, with average loss of 0.2177 in last 300 steps;\n",
      "Epoch 11, step 17100, with average loss of 0.2428 in last 300 steps;\n",
      "Epoch 11, step 17400, with average loss of 0.2879 in last 300 steps;\n",
      "Epoch 11, finish training took 00: 00: 07;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-80cba39e005f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                     \u001b[0mpredictions_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_s\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                     \u001b[0mhr_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrr_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndcg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                     \u001b[0mHR_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhr_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3be58689130a>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, sess)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mlabel_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindice_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_replica_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindice_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[0mprediction_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindice_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprediction_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1331\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1333\u001b[1;33m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           return tf_session.TF_SessionRun_wrapper(session, options, feed_dict,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScopedTFStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#以下为训练部分####Training\n",
    "###################################################################################################\n",
    "def metrics_hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "def metrics_mrr(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(float(index + 1))\n",
    "    else:\n",
    "        return 0\n",
    "def metrics_ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0            \n",
    "def evaluate(predictions, labels):\n",
    "    label = int(labels[-1])\n",
    "    hr = metrics_hit(label, predictions)\n",
    "    mrr = metrics_mrr(label, predictions)\n",
    "    ndcg = metrics_ndcg(label, predictions)\n",
    "    return hr, mrr, ndcg\n",
    "if __name__ == '__main__':\n",
    "    with tf.Session() as sess:\n",
    "        train_data = tf.data.Dataset.from_tensor_slices(data_dict_str)#导入数据\n",
    "        train_data = train_data.shuffle(buffer_size=len(data_dict_str['user'])).batch(args.batch_size)\n",
    "        print('train_data',train_data)\n",
    "        print('train_data除以batch_size',len(data_dict_str['user'])/args.batch_size)\n",
    "        test_data = tf.data.Dataset.from_tensor_slices(data_dict_ste)\n",
    "        print('test_data',test_data) \n",
    "        test_data = test_data.batch(args.test_size + args.test_neg_num)\n",
    "        iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes) #迭代器\n",
    "        model = MF_model(args, iterator, num_users_s, num_items_s, True)#调用模型\n",
    "        print(\"Creating model with fresh parameters...\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        count = 0\n",
    "        loss = 0\n",
    "        last_count = 0\n",
    "        hr_s_list, mrr_s_list, ndcg_s_list = [], [], []\n",
    "        for epoch in range(1, args.epochs + 1):#一次迭代\n",
    "            print('=' * 30 + ' EPOCH %d ' % epoch + '=' * 30)\n",
    "            ################################## TRAINING ################################\n",
    "            if 6 > epoch > 3:\n",
    "                model.args.lr = 1e-3\n",
    "            if epoch >= 6:\n",
    "                model.args.lr = 1e-4\n",
    "            sess.run(model.iterator.make_initializer(train_data))#在这里给iterator迭代器输入数据\n",
    "            model.is_training = True\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                while True:\n",
    "                    count += 1\n",
    "                    loss += model.step(sess)#训练一次\n",
    "                    if count % 300 == 0:\n",
    "                        print('Epoch %d, step %d, with average loss of %.4f in last %d steps;'\n",
    "                              % (epoch, count, loss / (count - last_count), count - last_count))\n",
    "                        loss = 0\n",
    "                        last_count = count\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Epoch %d, finish training \" % epoch + \"took \" +\n",
    "                      time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)) + ';')\n",
    "            ################################## TESTING ################################\n",
    "            sess.run(model.iterator.make_initializer(test_data))\n",
    "            model.is_training = False\n",
    "            start_time = time.time()\n",
    "            HR_s, MRR_s, NDCG_s = [], [], []\n",
    "            predictions_s, labels_s = model.step(sess)\n",
    "            cnt = 1\n",
    "            try:\n",
    "                while True:\n",
    "                    predictions_s, labels_s= model.step(sess)\n",
    "                    hr_s, mrr_s, ndcg_s = evaluate(predictions_s, labels_s)\n",
    "                    HR_s.append(hr_s)\n",
    "                    MRR_s.append(mrr_s)\n",
    "                    NDCG_s.append(ndcg_s)\n",
    "                    cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                hr_s = np.array(HR_s).mean()\n",
    "                mrr_s = np.array(MRR_s).mean()\n",
    "                ndcg_s = np.array(NDCG_s).mean()\n",
    "                hr_s_list.append(hr_s)\n",
    "                mrr_s_list.append(mrr_s)\n",
    "                ndcg_s_list.append(ndcg_s)\n",
    "                print(\"Epoch %d, finish testing \" % epoch + \"took: \" +\n",
    "                      time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)) + ';')\n",
    "                print('Epoch %d, %s HR is %.4f, MRR is %.4f, NDCG is %.4f;' %\n",
    "                      (epoch, name_s, hr_s, mrr_s, ndcg_s))\n",
    "        print('=' * 30 + 'Finish training' + '=' * 30)\n",
    "        print('%s best HR is %.4f, MRR is %.4f, NDCG is %.4f;' %\n",
    "              (name_s, max(hr_s_list), max(mrr_s_list), max(ndcg_s_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4555d36",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
