{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4ebd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users_s 5130\n",
      "num_items_s 1685\n",
      "userNum_s 5130\n",
      "itemNum_s 1685\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "df_s = pd.read_csv('E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/Amazon_Instant_Video_noreview.csv', sep=',')\n",
    "file_path_s = 'E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/Amazon_Instant_Video_noreview.csv'\n",
    "path_s = '/'.join(file_path_s.split('/')[:-1])\n",
    "name_s = file_path_s.split('/')[-1].split('_')[0]\n",
    "train_npy_path_s = file_path_s.replace('.csv', '_train.npy')\n",
    "test_npy_path_s = file_path_s.replace('.csv', '_test.npy')\n",
    "def _load_data(df):\n",
    "    uuu=0\n",
    "    iii=0\n",
    "    for user, item in zip(df['uid'], df['iid']):\n",
    "        if uuu < user:\n",
    "            uuu = user\n",
    "        if iii < item:\n",
    "            iii = item        \n",
    "    return uuu+1,iii+1\n",
    "num_users_s, num_items_s = _load_data(df_s)\n",
    "print(\"num_users_s\",num_users_s)\n",
    "print(\"num_items_s\",num_items_s)\n",
    "def _construct_pos_dict(df):\n",
    "    pos_dict = defaultdict(set)\n",
    "    for user, item in zip(df['uid'], df['iid']):\n",
    "        pos_dict[user].add(item)\n",
    "    return pos_dict\n",
    "pos_dict_s=_construct_pos_dict(df_s)\n",
    "def read_data(dataPath):\n",
    "    inFile = open(dataPath + 'data_HGNR.pkl','rb')\n",
    "    data = pickle.load(inFile)\n",
    "    userNum, itemNum = data['userNum'], data['itemNum']\n",
    "    trainData, testData, testNegData  = data['trainData'], data['testData'], data['testNegData']\n",
    "    userUserSeq1Matrix,itemItemSeq1Matrix = data['userUserSeq1Matrix'], data['itemItemSeq1Matrix']\n",
    "    inFile.close()\n",
    "    return trainData, testData, testNegData,userUserSeq1Matrix,itemItemSeq1Matrix,userNum, itemNum\n",
    "dataPath1 = 'E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/'\n",
    "trainData_s, testData_s,testNegData_s,userUserSeq1Matrix,itemItemSeq1Matrix, userNum_s, itemNum_s = read_data(dataPath1)\n",
    "print('userNum_s',userNum_s)\n",
    "print('itemNum_s',itemNum_s)\n",
    "train_df_s1,train_df_s2 = [],[]\n",
    "test_df_s1,test_df_s2 = [],[]\n",
    "train_df_s,test_df_s = {'uid':[],'iid':[]},{'uid':[],'iid':[]}\n",
    "train_df_s,test_df_s =  pd.DataFrame(train_df_s),pd.DataFrame(test_df_s)\n",
    "for u,i in trainData_s:   \n",
    "    train_df_s1.append(u)\n",
    "    train_df_s2.append(i)\n",
    "train_df_s['uid'] = train_df_s1\n",
    "train_df_s['iid'] = train_df_s2 \n",
    "train_df_s.to_csv(path_s+'/%s_train_df.csv'% name_s, index=False)\n",
    "for u,i in testData_s:\n",
    "    test_df_s1.append(u)\n",
    "    test_df_s2.append(i)\n",
    "test_df_s['uid'] = test_df_s1\n",
    "test_df_s['iid'] = test_df_s2 \n",
    "test_df_s.to_csv(path_s+'/%s_test_df.csv'% name_s, index=False)\n",
    "def _add_negtive(user, item, num_items, pos_dict, neg_num, boolindex):\n",
    "    user, item, num_items, pos_dict, neg_num, train = user, item, num_items, pos_dict,neg_num ,boolindex\n",
    "    users, items, labels = [], [], []\n",
    "    neg_set = set(range(num_items)).difference(pos_dict[user])  #difference用于返回集合的差集，包含在第一个集合中，但不包含在第二个集合中#\n",
    "    neg_sample_list = np.random.choice(list(neg_set), neg_num, replace=False).tolist()\n",
    "    for neg_sample in neg_sample_list:\n",
    "        users.append(user)\n",
    "        items.append(neg_sample)\n",
    "        labels.append(0) if train == True else labels.append(neg_sample)\n",
    "    users.append(user)\n",
    "    items.append(item)\n",
    "    if train == True:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(int(item))\n",
    "    return (users, items, labels)\n",
    "\n",
    "users = []\n",
    "items = []\n",
    "labels = []    \n",
    "for user, item in zip(train_df_s['uid'], train_df_s['iid']):\n",
    "    batch_users, batch_items, batch_labels = _add_negtive(user, item, num_items_s, pos_dict_s,4 ,True)\n",
    "    users += batch_users\n",
    "    items += batch_items\n",
    "    labels += batch_labels\n",
    "\"\"\"\n",
    "str --> int\n",
    "results = ['1','2','3']\n",
    "results = list(map(int, results))    \n",
    "\"\"\"\n",
    "users = list(map(int, users))\n",
    "items = list(map(int, items))\n",
    "labels = list(map(int, labels))\n",
    "data_dict_str = {'user': users, 'item': items, 'label': labels}\n",
    "np.save(train_npy_path_s, data_dict_str)\n",
    "users = []\n",
    "items = []\n",
    "labels = []    \n",
    "for user, item in zip(test_df_s['uid'], test_df_s['iid']):\n",
    "    batch_users, batch_items, batch_labels = _add_negtive(user, item, num_items_s, pos_dict_s,99 ,False)\n",
    "    users += batch_users\n",
    "    items += batch_items\n",
    "    labels += batch_labels\n",
    "users = list(map(int, users))\n",
    "items = list(map(int, items))\n",
    "labels = list(map(int, labels))\n",
    "data_dict_ste = {'user': users, 'item': items, 'label': labels}\n",
    "np.save(test_npy_path_s, data_dict_ste)\n",
    "print(\"ok\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d378f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(NCForMF='MF', batch_size=128, cross_data_rebuild=False, data_rebuild=False, dropout_message=0, embedding_size=64, epochs=150, gnn_layers=[64, 64, 64], gpu_device=0, lr=0.005, mat_rebuild=False, mlp_layers=[32, 16, 8], processor_num=12, regularizer_rate=8e-05, test_neg_num=99, test_size=1, topK=10, train_neg_num=4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_device', type=int, default=0,\n",
    "                    help='choose which gpu to run')\n",
    "parser.add_argument('--cross_data_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild cross data')\n",
    "parser.add_argument('--data_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild train/test dataset')\n",
    "parser.add_argument('--mat_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild` adjacent mat')\n",
    "parser.add_argument('--processor_num', type=int, default=12,\n",
    "                    help='number of processors when preprocessing data')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help='size of mini-batch')\n",
    "parser.add_argument('--train_neg_num', type=int, default=4,\n",
    "                    help='number of negative samples per training positive sample')\n",
    "parser.add_argument('--test_size', type=int, default=1,\n",
    "                    help='size of sampled test data')\n",
    "parser.add_argument('--test_neg_num', type=int, default=99,\n",
    "                    help='number of negative samples for test')\n",
    "parser.add_argument('--epochs', type=int, default=150,\n",
    "                    help='the number of epochs')\n",
    "parser.add_argument('--gnn_layers', nargs='?', default=[64,64,64],\n",
    "                    help='the unit list of layers')\n",
    "parser.add_argument('--mlp_layers', nargs='?', default=[32,16,8],\n",
    "                    help='the unit list of layers')\n",
    "parser.add_argument('--embedding_size', type=int, default=64,\n",
    "                    help='the size for embedding user and item')\n",
    "parser.add_argument('--topK', type=int, default=10,\n",
    "                    help='topk for evaluation')\n",
    "parser.add_argument('--regularizer_rate', type=float, default=8e-5,   ####default=0.01\n",
    "                    help='the regularizer rate')\n",
    "parser.add_argument('--lr', type=float, default=0.005,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--dropout_message', type=float, default=0,  ####default=0.1\n",
    "                    help='dropout rate of message')\n",
    "parser.add_argument('--NCForMF', type=str, default='MF',\n",
    "                    help='method to propagate embeddings')\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.__version__\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os, sys, time\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "# from utils import metrics\n",
    "class ReviewGCN(object):\n",
    "    def __init__(self, args, iterator, norm_adj_mat1, norm_adj_mat2, num_users, num_items_s,is_training):\n",
    "        self.args = args\n",
    "        self.iterator = iterator\n",
    "        self.norm_adj_mat1 = norm_adj_mat1\n",
    "        self.norm_adj_mat2 = norm_adj_mat2\n",
    "        self.num_users = num_users\n",
    "        self.num_items_s = num_items_s\n",
    "        self.is_training = is_training\n",
    "        self.n_fold = 50\n",
    "        self.get_data()\n",
    "        self.all_weights = self.init_weights()\n",
    "        self.item_embeddings_s1, self.user_embeddings1 = self.create_lightgcn_embed1()\n",
    "        self.item_embeddings_s2, self.user_embeddings2 = self.create_lightgcn_embed2()\n",
    "        self.item_embeddings_s = self.item_embeddings_s1 + 0.1 * self.item_embeddings_s2\n",
    "        self.user_embeddings = self.user_embeddings1 + 0.1 * self.user_embeddings2\n",
    "\n",
    "        self.inference()\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "    def get_data(self):\n",
    "        sample = self.iterator.get_next()\n",
    "        self.user, self.item_s= sample['user'], sample['item']\n",
    "        self.label_s = tf.cast(sample['label'], tf.float32)\n",
    "    def init_weights(self):\n",
    "        all_weights = dict()\n",
    "        initializer = tf.truncated_normal_initializer(0.01)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.args.regularizer_rate)\n",
    "        all_weights['user_embeddings1'] = tf.get_variable(\n",
    "            'user_embeddings1', (self.num_users, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        all_weights['item_embeddings_s1'] = tf.get_variable(\n",
    "            'item_embeddings_s1', (self.num_items_s, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        all_weights['user_embeddings2'] = tf.get_variable(\n",
    "            'user_embeddings21', (self.num_users, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        all_weights['item_embeddings_s2'] = tf.get_variable(\n",
    "            'item_embeddings_s21', (self.num_items_s, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        self.layers_plus = [self.args.embedding_size] + self.args.gnn_layers\n",
    "        print(\"self.layers_plus\",self.layers_plus)\n",
    "        \n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            all_weights['W_gc1_%d' % k] = tf.get_variable(\n",
    "                'W_gc1_%d'% k, (self.layers_plus[k], self.layers_plus[k+ 1]), tf.float32, initializer, regularizer)\n",
    "            all_weights['b_gc1_%d' % k] = tf.get_variable(\n",
    "                'b_gc1_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)\n",
    "            all_weights['W_gc2_%d' % k] = tf.get_variable(\n",
    "                'W_gc2_%d'% k, (self.layers_plus[k], self.layers_plus[k+ 1]), tf.float32, initializer, regularizer)\n",
    "            all_weights['b_gc2_%d' % k] = tf.get_variable(\n",
    "                'b_gc2_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)            \n",
    "            \n",
    "            all_weights['W_mlp_%d' % k] = tf.get_variable(\n",
    "                'W_mlp_%d'% k, (self.layers_plus[k + 1], self.layers_plus[k + 1]), tf.float32, initializer, regularizer)\n",
    "            all_weights['b_mlp_%d' % k] = tf.get_variable(\n",
    "                'b_mlp_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)\n",
    "        return all_weights\n",
    "    #####################GCNGCNGCNGCNGCNGCNGCNGCNGCNGCN#############################################   \n",
    "    def creat_gcn_embedd1(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj_mat1)\n",
    "        embeddings = tf.concat([self.all_weights['item_embeddings_s1'], self.all_weights['user_embeddings1']], axis=0)\n",
    "        all_embeddings = [embeddings]\n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            temp_embedd = [tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings) for f in range(self.n_fold)]\n",
    "            embeddings = tf.concat(temp_embedd, axis=0)\n",
    "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.all_weights['W_gc1_%d'%k])\n",
    "                                          + self.all_weights['b_gc1_%d'%k])\n",
    "            embeddings = tf.nn.dropout(embeddings, 1 - self.args.dropout_message)\n",
    "            all_embeddings += [embeddings]\n",
    "        all_embeddings = tf.concat(all_embeddings, axis=1)\n",
    "        item_embeddings_s, user_embeddings = tf.split(all_embeddings, [self.num_items_s, self.num_users], axis=0)\n",
    "        return item_embeddings_s, user_embeddings\n",
    "    def creat_gcn_embedd2(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj_mat2)\n",
    "        embeddings = tf.concat([self.all_weights['item_embeddings_s2'], self.all_weights['user_embeddings2']], axis=0)\n",
    "        all_embeddings = [embeddings]\n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            temp_embedd = [tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings) for f in range(self.n_fold)]\n",
    "            embeddings = tf.concat(temp_embedd, axis=0)\n",
    "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.all_weights['W_gc2_%d'%k])\n",
    "                                          + self.all_weights['b_gc2_%d'%k])\n",
    "            embeddings = tf.nn.dropout(embeddings, 1 - self.args.dropout_message)\n",
    "            all_embeddings += [embeddings]\n",
    "        all_embeddings = tf.concat(all_embeddings, axis=1)\n",
    "        item_embeddings_s, user_embeddings = tf.split(all_embeddings, [self.num_items_s, self.num_users], axis=0)\n",
    "        return item_embeddings_s, user_embeddings  \n",
    "    \n",
    "    ########################lightGCNightGCNightGCNightGCN###############################################\n",
    "    def create_lightgcn_embed1(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj_mat1)  #Arating\n",
    "        ego_embeddings = tf.concat([self.all_weights['item_embeddings_s1'], self.all_weights['user_embeddings1']], axis=0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            ego_embeddings = side_embeddings\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        all_embeddings=tf.stack(all_embeddings,1)\n",
    "        all_embeddings=tf.reduce_mean(all_embeddings,axis=1,keepdims=False)\n",
    "        item_embeddings_s, user_embeddings = tf.split(all_embeddings, [self.num_items_s, self.num_users], axis=0)\n",
    "        return item_embeddings_s, user_embeddings\n",
    "    def create_lightgcn_embed2(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj_mat2) #Areview \n",
    "        ego_embeddings = tf.concat([self.all_weights['item_embeddings_s2'], self.all_weights['user_embeddings2']], axis=0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            ego_embeddings = side_embeddings\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        all_embeddings=tf.stack(all_embeddings,1)\n",
    "        all_embeddings=tf.reduce_mean(all_embeddings,axis=1,keepdims=False)\n",
    "        item_embeddings_s, user_embeddings = tf.split(all_embeddings, [self.num_items_s, self.num_users], axis=0)\n",
    "        return item_embeddings_s, user_embeddings\n",
    "    ################################################################################################# \n",
    "    def _split_A_hat(self, X):\n",
    "        fold_len = math.ceil((X.shape[0]) / self.n_fold)\n",
    "        A_fold_hat = [self._convert_sp_mat_to_sp_tensor( X[i_fold*fold_len :(i_fold+1)*fold_len])\n",
    "                      for i_fold in range(self.n_fold)]\n",
    "\n",
    "        return A_fold_hat\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    def inference(self):      \n",
    "        initializer = tf.truncated_normal_initializer(0.01)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.args.regularizer_rate)\n",
    "        with tf.name_scope('embedding'):\n",
    "            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user)\n",
    "            item_embedding_s = tf.nn.embedding_lookup(self.item_embeddings_s, self.item_s)\n",
    "        with tf.name_scope('propagation'):\n",
    "            if self.args.NCForMF == 'MF':\n",
    "                self.logits_dense_s = tf.reduce_sum(tf.multiply(user_embedding, item_embedding_s), 1)  #logits_dense_s =256*1#\n",
    "            elif self.args.NCForMF == 'NCF':\n",
    "                a_s = tf.concat([user_embedding, item_embedding_s], axis=-1, name='inputs_s')\n",
    "                for i, units in enumerate(self.args.mlp_layers):\n",
    "                    dense_s = tf.layers.dense(a_s, units, tf.nn.relu, kernel_initializer=initializer,\n",
    "                                          kernel_regularizer = regularizer, name='dense_s_%d' % i)\n",
    "                    a_s = tf.layers.dropout(dense_s, self.args.dropout_message)\n",
    "                self.logits_dense_s = tf.layers.dense(inputs=a_s,\n",
    "                                                      units=1,\n",
    "                                                      kernel_initializer=initializer,\n",
    "                                                      kernel_regularizer=regularizer,\n",
    "                                                      name='logits_dense_s')\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "            self.logits_s = tf.squeeze(self.logits_dense_s)\n",
    "\n",
    "            loss_list_s = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_s, logits=self.logits_s,     #交叉熵#\n",
    "                                                                  name='loss_s')\n",
    "            loss_w_s = tf.map_fn(lambda x: tf.cond(tf.equal(x, 1.0), lambda: 5.0, lambda: 1.0), self.label_s)\n",
    "\n",
    "            self.loss_s = tf.reduce_mean(tf.multiply(loss_list_s, loss_w_s))\n",
    "            self.loss = self.loss_s \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.args.lr).minimize(self.loss)\n",
    "            self.label_replica_s = self.label_s\n",
    "            _, self.indice_s = tf.nn.top_k(tf.sigmoid(self.logits_s), self.args.topK)\n",
    "    def step(self, sess):\n",
    "        if self.is_training:\n",
    "            label_s, indice_s, loss, optim = sess.run(\n",
    "                [self.label_replica_s, self.indice_s, self.loss, self.optimizer])\n",
    "            return loss\n",
    "        else:\n",
    "            label_s, indice_s = sess.run([self.label_replica_s, self.indice_s])\n",
    "            prediction_s = np.take(label_s, indice_s)\n",
    "            return prediction_s, label_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d98993",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building adjacent mats_s..\n",
      "Get adjacent mats_s successfully.\n",
      "Building adjacent mats_s..\n",
      "Get adjacent mats_s successfully.\n",
      "train_data <BatchDataset shapes: {user: (?,), item: (?,), label: (?,)}, types: {user: tf.int32, item: tf.int32, label: tf.int32}>\n",
      "train_data除以batch_size 1249.84375\n",
      "test_data <TensorSliceDataset shapes: {user: (), item: (), label: ()}, types: {user: tf.int32, item: tf.int32, label: tf.int32}>\n",
      "self.layers_plus [64, 64, 64, 64]\n",
      "Creating model with fresh parameters...\n",
      "============================== EPOCH 1 ==============================\n",
      "Epoch 1, step 300, with average loss of 0.9277 in last 300 steps;\n",
      "Epoch 1, step 600, with average loss of 0.8560 in last 300 steps;\n",
      "Epoch 1, step 900, with average loss of 0.8045 in last 300 steps;\n",
      "Epoch 1, step 1200, with average loss of 0.7659 in last 300 steps;\n",
      "Epoch 1, finish training took 00: 03: 11;\n",
      "Epoch 1, finish testing took: 00: 01: 17;\n",
      "Epoch 1, Amazon HR is 0.3681, MRR is 0.1715, NDCG is 0.2175;\n",
      "============================== EPOCH 2 ==============================\n",
      "Epoch 2, step 1500, with average loss of 0.5700 in last 300 steps;\n",
      "Epoch 2, step 1800, with average loss of 0.5348 in last 300 steps;\n",
      "Epoch 2, step 2100, with average loss of 0.5237 in last 300 steps;\n",
      "Epoch 2, step 2400, with average loss of 0.5141 in last 300 steps;\n",
      "Epoch 2, finish training took 00: 03: 07;\n",
      "Epoch 2, finish testing took: 00: 01: 15;\n",
      "Epoch 2, Amazon HR is 0.4065, MRR is 0.1874, NDCG is 0.2388;\n",
      "============================== EPOCH 3 ==============================\n",
      "Epoch 3, step 2700, with average loss of 0.4051 in last 300 steps;\n",
      "Epoch 3, step 3000, with average loss of 0.3472 in last 300 steps;\n",
      "Epoch 3, step 3300, with average loss of 0.3455 in last 300 steps;\n",
      "Epoch 3, step 3600, with average loss of 0.3372 in last 300 steps;\n",
      "Epoch 3, finish training took 00: 03: 04;\n",
      "Epoch 3, finish testing took: 00: 01: 14;\n",
      "Epoch 3, Amazon HR is 0.4098, MRR is 0.1895, NDCG is 0.2412;\n",
      "============================== EPOCH 4 ==============================\n",
      "Epoch 4, step 3900, with average loss of 0.2784 in last 300 steps;\n",
      "Epoch 4, step 4200, with average loss of 0.2193 in last 300 steps;\n",
      "Epoch 4, step 4500, with average loss of 0.2156 in last 300 steps;\n",
      "Epoch 4, step 4800, with average loss of 0.2076 in last 300 steps;\n",
      "Epoch 4, finish training took 00: 03: 09;\n",
      "Epoch 4, finish testing took: 00: 01: 15;\n",
      "Epoch 4, Amazon HR is 0.4057, MRR is 0.1902, NDCG is 0.2409;\n",
      "============================== EPOCH 5 ==============================\n",
      "Epoch 5, step 5100, with average loss of 0.1828 in last 300 steps;\n",
      "Epoch 5, step 5400, with average loss of 0.1358 in last 300 steps;\n",
      "Epoch 5, step 5700, with average loss of 0.1271 in last 300 steps;\n",
      "Epoch 5, step 6000, with average loss of 0.1303 in last 300 steps;\n",
      "Epoch 5, finish training took 00: 03: 11;\n",
      "Epoch 5, finish testing took: 00: 01: 11;\n",
      "Epoch 5, Amazon HR is 0.4024, MRR is 0.1906, NDCG is 0.2405;\n",
      "============================== EPOCH 6 ==============================\n",
      "Epoch 6, step 6300, with average loss of 0.1206 in last 300 steps;\n",
      "Epoch 6, step 6600, with average loss of 0.0795 in last 300 steps;\n",
      "Epoch 6, step 6900, with average loss of 0.0805 in last 300 steps;\n",
      "Epoch 6, step 7200, with average loss of 0.0821 in last 300 steps;\n",
      "Epoch 6, step 7500, with average loss of 0.0853 in last 300 steps;\n",
      "Epoch 6, finish training took 00: 03: 06;\n",
      "Epoch 6, finish testing took: 00: 01: 16;\n",
      "Epoch 6, Amazon HR is 0.4046, MRR is 0.1900, NDCG is 0.2405;\n",
      "============================== EPOCH 7 ==============================\n",
      "Epoch 7, step 7800, with average loss of 0.0519 in last 300 steps;\n",
      "Epoch 7, step 8100, with average loss of 0.0550 in last 300 steps;\n",
      "Epoch 7, step 8400, with average loss of 0.0571 in last 300 steps;\n",
      "Epoch 7, step 8700, with average loss of 0.0562 in last 300 steps;\n",
      "Epoch 7, finish training took 00: 03: 08;\n",
      "Epoch 7, finish testing took: 00: 01: 12;\n",
      "Epoch 7, Amazon HR is 0.4018, MRR is 0.1903, NDCG is 0.2401;\n",
      "============================== EPOCH 8 ==============================\n",
      "Epoch 8, step 9000, with average loss of 0.0409 in last 300 steps;\n",
      "Epoch 8, step 9300, with average loss of 0.0406 in last 300 steps;\n",
      "Epoch 8, step 9600, with average loss of 0.0437 in last 300 steps;\n",
      "Epoch 8, step 9900, with average loss of 0.0450 in last 300 steps;\n",
      "Epoch 8, finish training took 00: 03: 07;\n",
      "Epoch 8, finish testing took: 00: 01: 14;\n",
      "Epoch 8, Amazon HR is 0.4073, MRR is 0.1885, NDCG is 0.2400;\n",
      "============================== EPOCH 9 ==============================\n",
      "Epoch 9, step 10200, with average loss of 0.0353 in last 300 steps;\n",
      "Epoch 9, step 10500, with average loss of 0.0300 in last 300 steps;\n",
      "Epoch 9, step 10800, with average loss of 0.0361 in last 300 steps;\n",
      "Epoch 9, step 11100, with average loss of 0.0386 in last 300 steps;\n",
      "Epoch 9, finish training took 00: 03: 08;\n",
      "Epoch 9, finish testing took: 00: 01: 14;\n",
      "Epoch 9, Amazon HR is 0.4104, MRR is 0.1931, NDCG is 0.2442;\n",
      "============================== EPOCH 10 ==============================\n",
      "Epoch 10, step 11400, with average loss of 0.0340 in last 300 steps;\n",
      "Epoch 10, step 11700, with average loss of 0.0275 in last 300 steps;\n",
      "Epoch 10, step 12000, with average loss of 0.0320 in last 300 steps;\n",
      "Epoch 10, step 12300, with average loss of 0.0366 in last 300 steps;\n",
      "Epoch 10, finish training took 00: 03: 11;\n",
      "Epoch 10, finish testing took: 00: 01: 16;\n",
      "Epoch 10, Amazon HR is 0.4147, MRR is 0.1958, NDCG is 0.2473;\n",
      "============================== EPOCH 11 ==============================\n",
      "Epoch 11, step 12600, with average loss of 0.0325 in last 300 steps;\n",
      "Epoch 11, step 12900, with average loss of 0.0242 in last 300 steps;\n",
      "Epoch 11, step 13200, with average loss of 0.0262 in last 300 steps;\n",
      "Epoch 11, step 13500, with average loss of 0.0313 in last 300 steps;\n",
      "Epoch 11, finish training took 00: 03: 11;\n",
      "Epoch 11, finish testing took: 00: 01: 14;\n",
      "Epoch 11, Amazon HR is 0.4176, MRR is 0.1963, NDCG is 0.2484;\n",
      "============================== EPOCH 12 ==============================\n",
      "Epoch 12, step 13800, with average loss of 0.0388 in last 300 steps;\n",
      "Epoch 12, step 14100, with average loss of 0.0211 in last 300 steps;\n",
      "Epoch 12, step 14400, with average loss of 0.0263 in last 300 steps;\n",
      "Epoch 12, step 14700, with average loss of 0.0343 in last 300 steps;\n",
      "Epoch 12, step 15000, with average loss of 0.0353 in last 300 steps;\n",
      "Epoch 12, finish training took 00: 03: 06;\n",
      "Epoch 12, finish testing took: 00: 01: 15;\n",
      "Epoch 12, Amazon HR is 0.4209, MRR is 0.1967, NDCG is 0.2494;\n",
      "============================== EPOCH 13 ==============================\n",
      "Epoch 13, step 15300, with average loss of 0.0231 in last 300 steps;\n",
      "Epoch 13, step 15600, with average loss of 0.0241 in last 300 steps;\n",
      "Epoch 13, step 15900, with average loss of 0.0291 in last 300 steps;\n",
      "Epoch 13, step 16200, with average loss of 0.0361 in last 300 steps;\n",
      "Epoch 13, finish training took 00: 03: 04;\n",
      "Epoch 13, finish testing took: 00: 01: 15;\n",
      "Epoch 13, Amazon HR is 0.4262, MRR is 0.2012, NDCG is 0.2541;\n",
      "============================== EPOCH 14 ==============================\n",
      "Epoch 14, step 16500, with average loss of 0.0245 in last 300 steps;\n",
      "Epoch 14, step 16800, with average loss of 0.0239 in last 300 steps;\n",
      "Epoch 14, step 17100, with average loss of 0.0269 in last 300 steps;\n",
      "Epoch 14, step 17400, with average loss of 0.0335 in last 300 steps;\n",
      "Epoch 14, finish training took 00: 03: 12;\n",
      "Epoch 14, finish testing took: 00: 01: 15;\n",
      "Epoch 14, Amazon HR is 0.4256, MRR is 0.1986, NDCG is 0.2520;\n",
      "============================== EPOCH 15 ==============================\n",
      "Epoch 15, step 17700, with average loss of 0.0252 in last 300 steps;\n",
      "Epoch 15, step 18000, with average loss of 0.0221 in last 300 steps;\n",
      "Epoch 15, step 18300, with average loss of 0.0270 in last 300 steps;\n",
      "Epoch 15, step 18600, with average loss of 0.0325 in last 300 steps;\n",
      "Epoch 15, finish training took 00: 03: 08;\n",
      "Epoch 15, finish testing took: 00: 01: 16;\n",
      "Epoch 15, Amazon HR is 0.4285, MRR is 0.2019, NDCG is 0.2552;\n",
      "============================== EPOCH 16 ==============================\n",
      "Epoch 16, step 18900, with average loss of 0.0282 in last 300 steps;\n",
      "Epoch 16, step 19200, with average loss of 0.0218 in last 300 steps;\n",
      "Epoch 16, step 19500, with average loss of 0.0255 in last 300 steps;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, step 19800, with average loss of 0.0333 in last 300 steps;\n",
      "Epoch 16, finish training took 00: 03: 04;\n",
      "Epoch 16, finish testing took: 00: 01: 15;\n",
      "Epoch 16, Amazon HR is 0.4363, MRR is 0.2055, NDCG is 0.2597;\n",
      "============================== EPOCH 17 ==============================\n",
      "Epoch 17, step 20100, with average loss of 0.0322 in last 300 steps;\n",
      "Epoch 17, step 20400, with average loss of 0.0207 in last 300 steps;\n",
      "Epoch 17, step 20700, with average loss of 0.0255 in last 300 steps;\n",
      "Epoch 17, step 21000, with average loss of 0.0304 in last 300 steps;\n",
      "Epoch 17, finish training took 00: 03: 05;\n",
      "Epoch 17, finish testing took: 00: 01: 14;\n",
      "Epoch 17, Amazon HR is 0.4373, MRR is 0.2043, NDCG is 0.2590;\n",
      "============================== EPOCH 18 ==============================\n",
      "Epoch 18, step 21300, with average loss of 0.0288 in last 300 steps;\n",
      "Epoch 18, step 21600, with average loss of 0.0179 in last 300 steps;\n",
      "Epoch 18, step 21900, with average loss of 0.0239 in last 300 steps;\n",
      "Epoch 18, step 22200, with average loss of 0.0289 in last 300 steps;\n",
      "Epoch 18, step 22500, with average loss of 0.0361 in last 300 steps;\n",
      "Epoch 18, finish training took 00: 03: 05;\n",
      "Epoch 18, finish testing took: 00: 01: 16;\n",
      "Epoch 18, Amazon HR is 0.4416, MRR is 0.2066, NDCG is 0.2619;\n",
      "============================== EPOCH 19 ==============================\n",
      "Epoch 19, step 22800, with average loss of 0.0237 in last 300 steps;\n",
      "Epoch 19, step 23100, with average loss of 0.0217 in last 300 steps;\n",
      "Epoch 19, step 23400, with average loss of 0.0286 in last 300 steps;\n",
      "Epoch 19, step 23700, with average loss of 0.0333 in last 300 steps;\n",
      "Epoch 19, finish training took 00: 03: 05;\n",
      "Epoch 19, finish testing took: 00: 01: 11;\n",
      "Epoch 19, Amazon HR is 0.4399, MRR is 0.2042, NDCG is 0.2596;\n",
      "============================== EPOCH 20 ==============================\n",
      "Epoch 20, step 24000, with average loss of 0.0223 in last 300 steps;\n",
      "Epoch 20, step 24300, with average loss of 0.0234 in last 300 steps;\n",
      "Epoch 20, step 24600, with average loss of 0.0237 in last 300 steps;\n",
      "Epoch 20, step 24900, with average loss of 0.0357 in last 300 steps;\n",
      "Epoch 20, finish training took 00: 02: 57;\n",
      "Epoch 20, finish testing took: 00: 01: 11;\n",
      "Epoch 20, Amazon HR is 0.4422, MRR is 0.2052, NDCG is 0.2609;\n",
      "============================== EPOCH 21 ==============================\n",
      "Epoch 21, step 25200, with average loss of 0.0244 in last 300 steps;\n",
      "Epoch 21, step 25500, with average loss of 0.0227 in last 300 steps;\n",
      "Epoch 21, step 25800, with average loss of 0.0237 in last 300 steps;\n",
      "Epoch 21, step 26100, with average loss of 0.0320 in last 300 steps;\n",
      "Epoch 21, finish training took 00: 03: 00;\n",
      "Epoch 21, finish testing took: 00: 01: 12;\n",
      "Epoch 21, Amazon HR is 0.4432, MRR is 0.2074, NDCG is 0.2629;\n",
      "============================== EPOCH 22 ==============================\n",
      "Epoch 22, step 26400, with average loss of 0.0279 in last 300 steps;\n",
      "Epoch 22, step 26700, with average loss of 0.0188 in last 300 steps;\n",
      "Epoch 22, step 27000, with average loss of 0.0246 in last 300 steps;\n",
      "Epoch 22, step 27300, with average loss of 0.0323 in last 300 steps;\n",
      "Epoch 22, finish training took 00: 03: 03;\n",
      "Epoch 22, finish testing took: 00: 01: 15;\n",
      "Epoch 22, Amazon HR is 0.4414, MRR is 0.2050, NDCG is 0.2606;\n",
      "============================== EPOCH 23 ==============================\n",
      "Epoch 23, step 27600, with average loss of 0.0292 in last 300 steps;\n",
      "Epoch 23, step 27900, with average loss of 0.0211 in last 300 steps;\n",
      "Epoch 23, step 28200, with average loss of 0.0236 in last 300 steps;\n",
      "Epoch 23, step 28500, with average loss of 0.0313 in last 300 steps;\n",
      "Epoch 23, finish training took 00: 03: 05;\n",
      "Epoch 23, finish testing took: 00: 01: 11;\n",
      "Epoch 23, Amazon HR is 0.4533, MRR is 0.2115, NDCG is 0.2683;\n",
      "============================== EPOCH 24 ==============================\n",
      "Epoch 24, step 28800, with average loss of 0.0318 in last 300 steps;\n",
      "Epoch 24, step 29100, with average loss of 0.0196 in last 300 steps;\n",
      "Epoch 24, step 29400, with average loss of 0.0198 in last 300 steps;\n",
      "Epoch 24, step 29700, with average loss of 0.0265 in last 300 steps;\n",
      "Epoch 24, step 30000, with average loss of 0.0361 in last 300 steps;\n",
      "Epoch 24, finish training took 00: 02: 57;\n",
      "Epoch 24, finish testing took: 00: 01: 11;\n",
      "Epoch 24, Amazon HR is 0.4490, MRR is 0.2091, NDCG is 0.2655;\n",
      "============================== EPOCH 25 ==============================\n",
      "Epoch 25, step 30300, with average loss of 0.0205 in last 300 steps;\n",
      "Epoch 25, step 30600, with average loss of 0.0209 in last 300 steps;\n",
      "Epoch 25, step 30900, with average loss of 0.0303 in last 300 steps;\n",
      "Epoch 25, step 31200, with average loss of 0.0340 in last 300 steps;\n",
      "Epoch 25, finish training took 00: 03: 03;\n",
      "Epoch 25, finish testing took: 00: 01: 11;\n",
      "Epoch 25, Amazon HR is 0.4568, MRR is 0.2129, NDCG is 0.2702;\n",
      "============================== EPOCH 26 ==============================\n",
      "Epoch 26, step 31500, with average loss of 0.0221 in last 300 steps;\n",
      "Epoch 26, step 31800, with average loss of 0.0214 in last 300 steps;\n",
      "Epoch 26, step 32100, with average loss of 0.0269 in last 300 steps;\n",
      "Epoch 26, step 32400, with average loss of 0.0319 in last 300 steps;\n",
      "Epoch 26, finish training took 00: 02: 54;\n",
      "Epoch 26, finish testing took: 00: 01: 10;\n",
      "Epoch 26, Amazon HR is 0.4519, MRR is 0.2094, NDCG is 0.2665;\n",
      "============================== EPOCH 27 ==============================\n",
      "Epoch 27, step 32700, with average loss of 0.0229 in last 300 steps;\n",
      "Epoch 27, step 33000, with average loss of 0.0195 in last 300 steps;\n",
      "Epoch 27, step 33300, with average loss of 0.0292 in last 300 steps;\n",
      "Epoch 27, step 33600, with average loss of 0.0308 in last 300 steps;\n",
      "Epoch 27, finish training took 00: 02: 54;\n",
      "Epoch 27, finish testing took: 00: 01: 10;\n",
      "Epoch 27, Amazon HR is 0.4560, MRR is 0.2138, NDCG is 0.2707;\n",
      "============================== EPOCH 28 ==============================\n",
      "Epoch 28, step 33900, with average loss of 0.0298 in last 300 steps;\n",
      "Epoch 28, step 34200, with average loss of 0.0166 in last 300 steps;\n",
      "Epoch 28, step 34500, with average loss of 0.0270 in last 300 steps;\n",
      "Epoch 28, step 34800, with average loss of 0.0292 in last 300 steps;\n",
      "Epoch 28, finish training took 00: 02: 53;\n",
      "Epoch 28, finish testing took: 00: 01: 09;\n",
      "Epoch 28, Amazon HR is 0.4558, MRR is 0.2117, NDCG is 0.2690;\n",
      "============================== EPOCH 29 ==============================\n",
      "Epoch 29, step 35100, with average loss of 0.0300 in last 300 steps;\n",
      "Epoch 29, step 35400, with average loss of 0.0171 in last 300 steps;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3201d1085c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m300\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         print('Epoch %d, step %d, with average loss of %.4f in last %d steps;'\n",
      "\u001b[1;32m<ipython-input-2-232660299e9f>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, sess)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             label_s, indice_s, loss, optim = sess.run(\n\u001b[1;32m--> 222\u001b[1;33m                 [self.label_replica_s, self.indice_s, self.loss, self.optimizer])\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def metrics_mrr(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(float(index + 1))\n",
    "    else:\n",
    "        return 0\n",
    "def metrics_hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "def metrics_ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0\n",
    "def evaluate(predictions, labels):\n",
    "    label = int(labels[-1])\n",
    "    hr = metrics_hit(label, predictions)\n",
    "    mrr = metrics_mrr(label, predictions)\n",
    "    ndcg = metrics_ndcg(label, predictions)\n",
    "    return hr, mrr, ndcg\n",
    "\n",
    "def normalized_adj_single(adj):\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    norm_adj = d_mat_inv.dot(adj)\n",
    "    return norm_adj\n",
    "\n",
    "def load_mat_s1(num_users_s, num_items_s, data_dict_str,userUserSeq1Matrix,itemItemSeq1Matrix, args):\n",
    "    print('Building adjacent mats_s..')  \n",
    "    num_users = num_users_s\n",
    "    num_items_s = num_items_s\n",
    "    train_df_s = {'user':data_dict_str['user'][args.train_neg_num::args.train_neg_num+1],\n",
    "                  'item':data_dict_str['item'][args.train_neg_num::args.train_neg_num+1]} \n",
    "\n",
    "    R_s = sp.dok_matrix((num_users, num_items_s), dtype=np.float32)\n",
    "    U_s = sp.dok_matrix(userUserSeq1Matrix, dtype=np.float32)\n",
    "    I_s = sp.dok_matrix(itemItemSeq1Matrix, dtype=np.float32)\n",
    "    for user, item in zip(train_df_s['user'], train_df_s['item']):\n",
    "        R_s[int(user), int(item)] = 1.0\n",
    "    plain_adj_mat = sp.dok_matrix((num_items_s+ num_users, num_items_s+ num_users),\n",
    "                                  dtype=np.float32).tolil()\n",
    "    plain_adj_mat[num_items_s: num_items_s+ num_users, :num_items_s] = R_s\n",
    "    plain_adj_mat[:num_items_s, num_items_s: num_items_s+ num_users] = R_s.T\n",
    "    plain_adj_mat[:num_items_s, :num_items_s] =  0.0*I_s\n",
    "    plain_adj_mat[num_items_s: num_items_s+ num_users, num_items_s: num_items_s+ num_users] =  0.0*U_s\n",
    "    plain_adj_mat = plain_adj_mat.todok()\n",
    "    norm_adj_mat_s1 = normalized_adj_single(plain_adj_mat+ sp.eye(plain_adj_mat.shape[0]))\n",
    "    print('Get adjacent mats_s successfully.')\n",
    "    return norm_adj_mat_s1\n",
    "def load_mat_s2(num_users_s, num_items_s, data_dict_str,userUserSeq1Matrix,itemItemSeq1Matrix, args):\n",
    "    print('Building adjacent mats_s..')  \n",
    "    num_users = num_users_s\n",
    "    num_items_s = num_items_s\n",
    "    train_df_s = {'user':data_dict_str['user'][args.train_neg_num::args.train_neg_num+1],\n",
    "                  'item':data_dict_str['item'][args.train_neg_num::args.train_neg_num+1]} \n",
    "\n",
    "    R_s = sp.dok_matrix((num_users, num_items_s), dtype=np.float32)\n",
    "    U_s = sp.dok_matrix(userUserSeq1Matrix, dtype=np.float32)\n",
    "    I_s = sp.dok_matrix(itemItemSeq1Matrix, dtype=np.float32)\n",
    "    for user, item in zip(train_df_s['user'], train_df_s['item']):\n",
    "        R_s[int(user), int(item)] = 1.0\n",
    "    plain_adj_mat = sp.dok_matrix((num_items_s+ num_users, num_items_s+ num_users),\n",
    "                                  dtype=np.float32).tolil()\n",
    "    plain_adj_mat[num_items_s: num_items_s+ num_users, :num_items_s] = 0.0*R_s\n",
    "    plain_adj_mat[:num_items_s, num_items_s: num_items_s+ num_users] = 0.0*R_s.T\n",
    "    plain_adj_mat[:num_items_s, :num_items_s] =  0.1*I_s\n",
    "    plain_adj_mat[num_items_s: num_items_s+ num_users, num_items_s: num_items_s+ num_users] =  0.1*U_s\n",
    "    plain_adj_mat = plain_adj_mat.todok()\n",
    "    norm_adj_mat_s2 = normalized_adj_single(plain_adj_mat+ sp.eye(plain_adj_mat.shape[0]))\n",
    "    print('Get adjacent mats_s successfully.')\n",
    "    return norm_adj_mat_s2\n",
    "##################################################################################################\n",
    "if __name__ == '__main__':\n",
    "    with tf.Session() as sess:\n",
    "        norm_adj_mat1 = load_mat_s1(num_users_s, num_items_s, data_dict_str,userUserSeq1Matrix,itemItemSeq1Matrix, args)\n",
    "        norm_adj_mat2 = load_mat_s2(num_users_s, num_items_s, data_dict_str,userUserSeq1Matrix,itemItemSeq1Matrix, args)\n",
    "        train_data = tf.data.Dataset.from_tensor_slices(data_dict_str)\n",
    "        train_data = train_data.shuffle(buffer_size=len(data_dict_str['user'])).batch(args.batch_size)\n",
    "        print('train_data',train_data)\n",
    "        print('train_data除以batch_size',len(data_dict_str['user'])/args.batch_size)\n",
    "        test_data = tf.data.Dataset.from_tensor_slices(data_dict_ste)\n",
    "        print('test_data',test_data) \n",
    "        test_data = test_data.batch(args.test_size + args.test_neg_num)\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "        model = ReviewGCN(args, iterator, norm_adj_mat1, norm_adj_mat2, num_users_s, num_items_s, True)\n",
    "        print(\"Creating model with fresh parameters...\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        count = 0\n",
    "        loss = 0\n",
    "        last_count = 0\n",
    "        hr_s_list, mrr_s_list, ndcg_s_list = [], [], []\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            print('=' * 30 + ' EPOCH %d ' % epoch + '=' * 30)\n",
    "            ################################## TRAINING ################################\n",
    "            if 6 > epoch > 3:\n",
    "                model.args.lr = 1e-3\n",
    "            if epoch >= 6:\n",
    "                model.args.lr = 1e-4\n",
    "            sess.run(model.iterator.make_initializer(train_data))\n",
    "            model.is_training = True\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                while True:\n",
    "                    count += 1\n",
    "                    loss += model.step(sess)\n",
    "                    if count % 300 == 0:\n",
    "                        print('Epoch %d, step %d, with average loss of %.4f in last %d steps;'\n",
    "                              % (epoch, count, loss / (count - last_count), count - last_count))\n",
    "                        loss = 0\n",
    "                        last_count = count\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Epoch %d, finish training \" % epoch + \"took \" +\n",
    "                      time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)) + ';')\n",
    "\n",
    "            ################################## TESTING ################################\n",
    "            sess.run(model.iterator.make_initializer(test_data))\n",
    "            model.is_training = False\n",
    "            start_time = time.time()\n",
    "            HR_s, MRR_s, NDCG_s = [], [], []\n",
    "            predictions_s, labels_s = model.step(sess)\n",
    "\n",
    "            cnt = 1\n",
    "            try:\n",
    "                while True:\n",
    "                    predictions_s, labels_s= model.step(sess)\n",
    "                    hr_s, mrr_s, ndcg_s = evaluate(predictions_s, labels_s)\n",
    "                    HR_s.append(hr_s)\n",
    "                    MRR_s.append(mrr_s)\n",
    "                    NDCG_s.append(ndcg_s)\n",
    "                    cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                hr_s = np.array(HR_s).mean()\n",
    "                mrr_s = np.array(MRR_s).mean()\n",
    "                ndcg_s = np.array(NDCG_s).mean()\n",
    "                hr_s_list.append(hr_s)\n",
    "                mrr_s_list.append(mrr_s)\n",
    "                ndcg_s_list.append(ndcg_s)\n",
    "                print(\"Epoch %d, finish testing \" % epoch + \"took: \" +\n",
    "                      time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)) + ';')\n",
    "                print('Epoch %d, %s HR is %.4f, MRR is %.4f, NDCG is %.4f;' %\n",
    "                      (epoch, name_s, hr_s, mrr_s, ndcg_s))\n",
    "        print('=' * 30 + 'Finish training' + '=' * 30)\n",
    "        print('%s best HR is %.4f, MRR is %.4f, NDCG is %.4f;' %\n",
    "              (name_s, max(hr_s_list), max(mrr_s_list), max(ndcg_s_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f82673",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
