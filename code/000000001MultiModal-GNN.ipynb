{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b2dfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users_s 5130\n",
      "num_items_s 1685\n",
      "userNum_s 5130\n",
      "itemNum_s 1685\n"
     ]
    }
   ],
   "source": [
    "#以下为数据预处理部分####data_preprocess\n",
    "###################################################################################################\n",
    "from pandas import Series, DataFrame\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "df_s = pd.read_csv(\"E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/Amazon_Instant_Video_noreview.csv\", sep=',')\n",
    "file_path_s =\"E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/Amazon_Instant_Video_noreview.csv\"\n",
    "path_s = '/'.join(file_path_s.split('/')[:-1])\n",
    "name_s = file_path_s.split('/')[-1].split('_')[0]\n",
    "train_npy_path_s = file_path_s.replace('.csv', '_train.npy')\n",
    "test_npy_path_s = file_path_s.replace('.csv', '_test.npy')\n",
    "def _load_data(df):\n",
    "    uuu=0\n",
    "    iii=0\n",
    "    for user, item in zip(df['uid'], df['iid']):\n",
    "        if uuu < user:\n",
    "            uuu = user\n",
    "        if iii < item:\n",
    "            iii = item        \n",
    "    return uuu+1,iii+1\n",
    "num_users_s, num_items_s = _load_data(df_s)\n",
    "print(\"num_users_s\",num_users_s)\n",
    "print(\"num_items_s\",num_items_s)\n",
    "\n",
    "def _construct_pos_dict(df):\n",
    "    pos_dict = defaultdict(set)\n",
    "    for user, item in zip(df['uid'], df['iid']):\n",
    "        pos_dict[user].add(item)\n",
    "    return pos_dict\n",
    "pos_dict_s=_construct_pos_dict(df_s)\n",
    "\n",
    "def read_data(dataPath):\n",
    "    inFile = open(dataPath + 'data_HGNR.pkl','rb')\n",
    "    data = pickle.load(inFile)\n",
    "    userNum, itemNum = data['userNum'], data['itemNum']\n",
    "    trainData, testData, testNegData  = data['trainData'], data['testData'], data['testNegData']\n",
    "    inFile.close()\n",
    "    return trainData, testData, testNegData, userNum, itemNum\n",
    "dataPath1 = \"E:/Charlotte/CS/MACHINE LEARNING/RECOMMENDATION/Amazon_Instant_Video_noreview/\"\n",
    "trainData_s, testData_s,testNegData_s, userNum_s, itemNum_s = read_data(dataPath1)\n",
    "print('userNum_s',userNum_s)\n",
    "print('itemNum_s',itemNum_s)\n",
    "train_df_s1,train_df_s2 = [],[]\n",
    "test_df_s1,test_df_s2 = [],[]\n",
    "train_df_s,test_df_s = {'uid':[],'iid':[]},{'uid':[],'iid':[]}\n",
    "train_df_s,test_df_s =  pd.DataFrame(train_df_s),pd.DataFrame(test_df_s)\n",
    "for u,i in trainData_s:   \n",
    "    train_df_s1.append(u)\n",
    "    train_df_s2.append(i)\n",
    "train_df_s['uid'] = train_df_s1\n",
    "train_df_s['iid'] = train_df_s2 \n",
    "train_df_s.to_csv(path_s+'/%s_train_df.csv'% name_s, index=False)\n",
    "for u,i in testData_s:\n",
    "    test_df_s1.append(u)\n",
    "    test_df_s2.append(i)\n",
    "test_df_s['uid'] = test_df_s1\n",
    "test_df_s['iid'] = test_df_s2 \n",
    "test_df_s.to_csv(path_s+'/%s_test_df.csv'% name_s, index=False)\n",
    "\n",
    "def _add_negtive(user, item, num_items, pos_dict, neg_num, boolindex):\n",
    "    user, item, num_items, pos_dict, neg_num, train = user, item, num_items, pos_dict,neg_num ,boolindex\n",
    "    users, items, labels = [], [], []\n",
    "    neg_set = set(range(num_items)).difference(pos_dict[user])  #difference用于返回集合的差集，包含在第一个集合中，但不包含在第二个集合中#\n",
    "    neg_sample_list = np.random.choice(list(neg_set), neg_num, replace=False).tolist()\n",
    "    for neg_sample in neg_sample_list:\n",
    "        users.append(user)\n",
    "        items.append(neg_sample)\n",
    "        labels.append(0) if train == True else labels.append(neg_sample)\n",
    "    users.append(user)\n",
    "    items.append(item)\n",
    "    if train == True:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(int(item))\n",
    "    return (users, items, labels)\n",
    "\n",
    "users = []\n",
    "items = []\n",
    "labels = []    \n",
    "for user, item in zip(train_df_s['uid'], train_df_s['iid']):\n",
    "    batch_users, batch_items, batch_labels = _add_negtive(user, item, num_items_s, pos_dict_s,4 ,True)\n",
    "    users += batch_users\n",
    "    items += batch_items\n",
    "    labels += batch_labels\n",
    "users = list(map(int, users))\n",
    "items = list(map(int, items))\n",
    "labels = list(map(int, labels))\n",
    "data_dict_str = {'user': users, 'item': items, 'label': labels}\n",
    "np.save(train_npy_path_s, data_dict_str)\n",
    "users = []\n",
    "items = []\n",
    "labels = []    \n",
    "for user, item in zip(test_df_s['uid'], test_df_s['iid']):\n",
    "    batch_users, batch_items, batch_labels = _add_negtive(user, item, num_items_s, pos_dict_s,99 ,False)\n",
    "    users += batch_users\n",
    "    items += batch_items\n",
    "    labels += batch_labels\n",
    "users = list(map(int, users))\n",
    "items = list(map(int, items))\n",
    "labels = list(map(int, labels))\n",
    "data_dict_ste = {'user': users, 'item': items, 'label': labels}\n",
    "np.save(test_npy_path_s, data_dict_ste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cd020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(NCForMF='MF', batch_size=128, cross_data_rebuild=False, data_rebuild=False, dropout_message=0, embedding_size=64, epochs=150, gnn_layers=[64, 64, 64, 64, 64, 64, 64, 64], gpu_device=0, lr=0.005, mat_rebuild=False, mlp_layers=[32, 16, 8], processor_num=12, regularizer_rate=8e-05, test_neg_num=99, test_size=1, topK=10, train_neg_num=4)\n"
     ]
    }
   ],
   "source": [
    "#以下为模型部分####Model\n",
    "###################################################################################################\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow\n",
    "tensorflow.__version__\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "import os, sys, time\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_device', type=int, default=0,\n",
    "                    help='choose which gpu to run')\n",
    "parser.add_argument('--cross_data_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild cross data')\n",
    "parser.add_argument('--data_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild train/test dataset')\n",
    "parser.add_argument('--mat_rebuild', type=bool, default=False,\n",
    "                    help='whether to rebuild` adjacent mat')\n",
    "parser.add_argument('--processor_num', type=int, default=12,\n",
    "                    help='number of processors when preprocessing data')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help='size of mini-batch')\n",
    "parser.add_argument('--train_neg_num', type=int, default=4,\n",
    "                    help='number of negative samples per training positive sample')\n",
    "parser.add_argument('--test_size', type=int, default=1,\n",
    "                    help='size of sampled test data')\n",
    "parser.add_argument('--test_neg_num', type=int, default=99,\n",
    "                    help='number of negative samples for test')\n",
    "parser.add_argument('--epochs', type=int, default=150,\n",
    "                    help='the number of epochs')\n",
    "parser.add_argument('--gnn_layers', nargs='?', default=[64,64,64],\n",
    "                    help='the unit list of layers')#gnn layer是说参数W是64维的，即经转换后64位的embedding向量还是64位的\n",
    "parser.add_argument('--mlp_layers', nargs='?', default=[32,16,8],\n",
    "                    help='the unit list of layers')\n",
    "parser.add_argument('--embedding_size', type=int, default=64,\n",
    "                    help='the size for embedding user and item') #embedding_size是说每个item和商品的embedding是64位的一维向量\n",
    "parser.add_argument('--topK', type=int, default=10,\n",
    "                    help='topk for evaluation')\n",
    "parser.add_argument('--regularizer_rate', type=float, default=8e-5,   \n",
    "                    help='the regularizer rate')\n",
    "parser.add_argument('--lr', type=float, default=0.005,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--dropout_message', type=float, default=0, \n",
    "                    help='dropout rate of message')\n",
    "parser.add_argument('--NCForMF', type=str, default='MF',\n",
    "                    help='method to propagate embeddings')\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "class GNN(object):\n",
    "    def __init__(self, args, iterator, norm_adj_mat1, num_users, num_items_s,is_training):\n",
    "        self.args = args\n",
    "        self.iterator = iterator\n",
    "        self.norm_adj_mat1 = norm_adj_mat1\n",
    "        self.num_users = num_users\n",
    "        self.num_items_s = num_items_s\n",
    "        self.is_training = is_training\n",
    "        self.n_fold = 50\n",
    "        self.get_data()\n",
    "        self.all_weights = self.init_weights()\n",
    "        ####调用GNN模型################################################################\n",
    "        self.item_embeddings_s1, self.user_embeddings1 = self.create_lightgcn_embed1()\n",
    "        self.item_embeddings_s = self.item_embeddings_s1 \n",
    "        self.user_embeddings = self.user_embeddings1 \n",
    "        \n",
    "        self.inference()\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "    def get_data(self):\n",
    "        sample = self.iterator.get_next()\n",
    "        self.user, self.item_s= sample['user'], sample['item']\n",
    "        self.label_s = tf.cast(sample['label'], tf.float32)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        all_weights = dict()\n",
    "        initializer = tf.truncated_normal_initializer(0.01)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.args.regularizer_rate)\n",
    "        all_weights['user_embeddings1'] = tf.get_variable(\n",
    "            'user_embeddings1', (self.num_users, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        all_weights['item_embeddings_s1'] = tf.get_variable(\n",
    "            'item_embeddings_s1', (self.num_items_s, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "        all_weights['user_embeddings21'] = tf.get_variable(\n",
    "            'user_embeddings21', (self.num_users, self.args.embedding_size), tf.float32, initializer, regularizer)\n",
    "           \n",
    "        self.layers_plus = [self.args.embedding_size] + self.args.gnn_layers\n",
    "\n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            all_weights['W_gc1_%d' % k] = tf.get_variable(\n",
    "                'W_gc1_%d'% k, (self.layers_plus[k], self.layers_plus[k+ 1]), tf.float32, initializer, regularizer)\n",
    "                #W的行数就是embedding的大小即64，列数就是gnnlayer的大小，意思是乘完之后就把embedding的大小改为gnnlayer（并不是说gnn有这么多层，gnn只有一层）\n",
    "            all_weights['b_gc1_%d' % k] = tf.get_variable(\n",
    "                'b_gc1_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)\n",
    "            all_weights['W_gc2_%d' % k] = tf.get_variable(\n",
    "                'W_gc2_%d'% k, (self.layers_plus[k], self.layers_plus[k+ 1]), tf.float32, initializer, regularizer)\n",
    "            all_weights['b_gc2_%d' % k] = tf.get_variable(\n",
    "                'b_gc2_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)            \n",
    "            \n",
    "            all_weights['W_bi_%d' % k] = tf.get_variable(\n",
    "                'W_bi_%d'% k, (self.layers_plus[k], self.layers_plus[k + 1]), tf.float32, initializer, regularizer)\n",
    "            all_weights['b_bi_%d' % k] = tf.get_variable(\n",
    "                'b_bi_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)\n",
    "            all_weights['W_mlp_%d' % k] = tf.get_variable(\n",
    "                'W_mlp_%d'% k, (self.layers_plus[k + 1], self.layers_plus[k + 1]), tf.float32, initializer, regularizer)\n",
    "            all_weights['b_mlp_%d' % k] = tf.get_variable(\n",
    "                'b_mlp_%d'% k, self.layers_plus[k+ 1], tf.float32, tf.zeros_initializer(), regularizer)\n",
    "        return all_weights\n",
    "    ####################GC-MC####################################  \n",
    "    \n",
    "    def create_gcmc_embed(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj_mat1)\n",
    "        embeddings = tf.concat([self.all_weights['item_embeddings_s1'], self.all_weights['user_embeddings1']], axis=0)\n",
    "        all_embeddings = []\n",
    "        for k in range(len(self.layers_plus)-1):#[layers_plus=[64,64]]是说gnn只有一层\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):#把adjacency矩阵裂开来运算是为了节省内存空间\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))#可以再解释一下那个embedding矩阵和adjacency稀疏矩阵相乘的原理吗？\n",
    "            embeddings = tf.concat(temp_embed, 0)\n",
    "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings,self.all_weights['W_gc1_%d' % k])+self.all_weights['b_gc1_%d' % k])\n",
    "            embeddings = tf.nn.dropout(embeddings,1-self.args.dropout_message)\n",
    "            all_embeddings += [embeddings]\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "        item_embeddings_s, user_embeddings = tf.split(all_embeddings, [self.num_items_s, self.num_users], axis=0)\n",
    "        return item_embeddings_s, user_embeddings\n",
    "    \n",
    "    def create_lightgcn_embed1(self):\n",
    "        A_fold_hat = self._split_A_hat(self.norm_adj_mat1)  \n",
    "        ego_embeddings = tf.concat([self.all_weights['item_embeddings_s1'], self.all_weights['user_embeddings1']], axis=0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "        for k in range(len(self.layers_plus)-1):\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "            ego_embeddings = side_embeddings\n",
    "            all_embeddings += [ego_embeddings]\n",
    "        all_embeddings=tf.stack(all_embeddings,1)\n",
    "        all_embeddings=tf.reduce_mean(all_embeddings,axis=1,keepdims=False)\n",
    "        item_embeddings_s, user_embeddings = tf.split(all_embeddings, [self.num_items_s, self.num_users], axis=0)\n",
    "        return item_embeddings_s, user_embeddings\n",
    "    \n",
    "    def _split_A_hat(self, X):\n",
    "        fold_len = math.ceil((X.shape[0]) / self.n_fold)\n",
    "        A_fold_hat = [self._convert_sp_mat_to_sp_tensor( X[i_fold*fold_len :(i_fold+1)*fold_len])\n",
    "                      for i_fold in range(self.n_fold)]\n",
    "        return A_fold_hat\n",
    "    \n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    \n",
    "    def inference(self):    \n",
    "        initializer = tf.truncated_normal_initializer(0.01)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.args.regularizer_rate)\n",
    "        with tf.name_scope('embedding'):\n",
    "            user_embedding = tf.nn.embedding_lookup(self.user_embeddings, self.user)\n",
    "            item_embedding_s = tf.nn.embedding_lookup(self.item_embeddings_s, self.item_s)\n",
    "        with tf.name_scope('propagation'):\n",
    "            if self.args.NCForMF == 'MF':\n",
    "                self.logits_dense_s = tf.reduce_sum(tf.multiply(user_embedding, item_embedding_s), 1)  #logits_dense_s =256*1#\n",
    "            elif self.args.NCForMF == 'NCF':\n",
    "                a_s = tf.concat([user_embedding, item_embedding_s], axis=-1, name='inputs_s')\n",
    "                for i, units in enumerate(self.args.mlp_layers):\n",
    "                    dense_s = tf.layers.dense(a_s, units, tf.nn.relu, kernel_initializer=initializer,\n",
    "                                          kernel_regularizer = regularizer, name='dense_s_%d' % i)\n",
    "                    a_s = tf.layers.dropout(dense_s, self.args.dropout_message)\n",
    "                self.logits_dense_s = tf.layers.dense(inputs=a_s,\n",
    "                                                      units=1,\n",
    "                                                      kernel_initializer=initializer,\n",
    "                                                      kernel_regularizer=regularizer,\n",
    "                                                      name='logits_dense_s')\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "            self.logits_s = tf.squeeze(self.logits_dense_s)\n",
    "\n",
    "            loss_list_s = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label_s, logits=self.logits_s,     #交叉熵#\n",
    "                                                                  name='loss_s')\n",
    "            loss_w_s = tf.map_fn(lambda x: tf.cond(tf.equal(x, 1.0), lambda: 5.0, lambda: 1.0), self.label_s)\n",
    "\n",
    "            self.loss_s = tf.reduce_mean(tf.multiply(loss_list_s, loss_w_s))\n",
    "            self.loss = self.loss_s \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.args.lr).minimize(self.loss)\n",
    "            self.label_replica_s = self.label_s\n",
    "            _, self.indice_s = tf.nn.top_k(tf.sigmoid(self.logits_s), self.args.topK)\n",
    "            \n",
    "    def step(self, sess):\n",
    "        if self.is_training:\n",
    "            label_s, indice_s, loss, optim = sess.run(\n",
    "                [self.label_replica_s, self.indice_s, self.loss, self.optimizer])\n",
    "            return loss\n",
    "        else:\n",
    "            label_s, indice_s = sess.run([self.label_replica_s, self.indice_s])\n",
    "            prediction_s = np.take(label_s, indice_s)\n",
    "            return prediction_s, label_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea846252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building adjacent mats_s..\n",
      "Get adjacent mats_s successfully.\n",
      "train_data <BatchDataset shapes: {user: (?,), item: (?,), label: (?,)}, types: {user: tf.int32, item: tf.int32, label: tf.int32}>\n",
      "train_data除以batch_size 1249.84375\n",
      "test_data <TensorSliceDataset shapes: {user: (), item: (), label: ()}, types: {user: tf.int32, item: tf.int32, label: tf.int32}>\n",
      "Creating model with fresh parameters...\n",
      "============================== EPOCH 1 ==============================\n",
      "Epoch 1, step 300, with average loss of 1.0177 in last 300 steps;\n",
      "Epoch 1, step 600, with average loss of 0.8658 in last 300 steps;\n",
      "Epoch 1, step 900, with average loss of 0.8062 in last 300 steps;\n",
      "Epoch 1, step 1200, with average loss of 0.7694 in last 300 steps;\n",
      "Epoch 1, finish training took 00: 05: 07;\n",
      "Epoch 1, finish testing took: 00: 02: 25;\n",
      "Epoch 1, Amazon HR is 0.5549, MRR is 0.2655, NDCG is 0.3340;\n",
      "============================== EPOCH 2 ==============================\n",
      "Epoch 2, step 1500, with average loss of 0.7021 in last 300 steps;\n",
      "Epoch 2, step 1800, with average loss of 0.6715 in last 300 steps;\n",
      "Epoch 2, step 2100, with average loss of 0.6633 in last 300 steps;\n",
      "Epoch 2, step 2400, with average loss of 0.6534 in last 300 steps;\n",
      "Epoch 2, finish training took 00: 31: 49;\n",
      "Epoch 2, finish testing took: 00: 03: 21;\n",
      "Epoch 2, Amazon HR is 0.5767, MRR is 0.2831, NDCG is 0.3527;\n",
      "============================== EPOCH 3 ==============================\n",
      "Epoch 3, step 2700, with average loss of 0.6049 in last 300 steps;\n",
      "Epoch 3, step 3000, with average loss of 0.5784 in last 300 steps;\n",
      "Epoch 3, step 3300, with average loss of 0.5737 in last 300 steps;\n",
      "Epoch 3, step 3600, with average loss of 0.5660 in last 300 steps;\n",
      "Epoch 3, finish training took 00: 10: 24;\n"
     ]
    }
   ],
   "source": [
    "### 以下为训练部分####Training###################################################################################################\n",
    "def metrics_hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "def metrics_mrr(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(float(index + 1))\n",
    "    else:\n",
    "        return 0\n",
    "def metrics_ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0            \n",
    "def evaluate(predictions, labels):\n",
    "    label = int(labels[-1])\n",
    "    hr = metrics_hit(label, predictions)\n",
    "    mrr = metrics_mrr(label, predictions)\n",
    "    ndcg = metrics_ndcg(label, predictions)\n",
    "    return hr, mrr, ndcg\n",
    "\n",
    "def normalized_adj_single(adj):\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv = np.power(rowsum, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    norm_adj = d_mat_inv.dot(adj)\n",
    "    return norm_adj\n",
    "\n",
    "def load_mat_s1(num_users_s, num_items_s, data_dict_str, args):\n",
    "    print('Building adjacent mats_s..')  \n",
    "    num_users = num_users_s\n",
    "    num_items_s = num_items_s\n",
    "    train_df_s = {'user':data_dict_str['user'][args.train_neg_num::args.train_neg_num+1],\n",
    "                  'item':data_dict_str['item'][args.train_neg_num::args.train_neg_num+1]} \n",
    "\n",
    "    R_s = sp.dok_matrix((num_users, num_items_s), dtype=np.float32)\n",
    "\n",
    "    for user, item in zip(train_df_s['user'], train_df_s['item']):\n",
    "        R_s[int(user), int(item)] = 1.0\n",
    "    plain_adj_mat = sp.dok_matrix((num_items_s+ num_users, num_items_s+ num_users),\n",
    "                                  dtype=np.float32).tolil()\n",
    "    plain_adj_mat[num_items_s: num_items_s+ num_users, :num_items_s] = R_s\n",
    "    plain_adj_mat[:num_items_s, num_items_s: num_items_s+ num_users] = R_s.T\n",
    "    plain_adj_mat[:num_items_s, :num_items_s] =  0.0\n",
    "    plain_adj_mat[num_items_s: num_items_s+ num_users, num_items_s: num_items_s+ num_users] =  0.0\n",
    "    plain_adj_mat = plain_adj_mat.todok()\n",
    "    norm_adj_mat_s1 = normalized_adj_single(plain_adj_mat+ sp.eye(plain_adj_mat.shape[0]))\n",
    "    print('Get adjacent mats_s successfully.')\n",
    "    return norm_adj_mat_s1\n",
    "\n",
    "##################################################################################################\n",
    "if __name__ == '__main__':\n",
    "    with tf.Session() as sess:\n",
    "        norm_adj_mat1 = load_mat_s1(num_users_s, num_items_s, data_dict_str, args)\n",
    "        train_data = tf.data.Dataset.from_tensor_slices(data_dict_str)\n",
    "        train_data = train_data.shuffle(buffer_size=len(data_dict_str['user'])).batch(args.batch_size)\n",
    "        print('train_data',train_data)\n",
    "        print('train_data除以batch_size',len(data_dict_str['user'])/args.batch_size)\n",
    "        test_data = tf.data.Dataset.from_tensor_slices(data_dict_ste)\n",
    "        print('test_data',test_data) \n",
    "        test_data = test_data.batch(args.test_size + args.test_neg_num)\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "        model = GNN(args, iterator, norm_adj_mat1, num_users_s, num_items_s, True)\n",
    "        print(\"Creating model with fresh parameters...\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        count = 0\n",
    "        loss = 0\n",
    "        last_count = 0\n",
    "        hr_s_list, mrr_s_list, ndcg_s_list = [], [], []\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            print('=' * 30 + ' EPOCH %d ' % epoch + '=' * 30)\n",
    "            ################################## TRAINING ################################\n",
    "            if 6 > epoch > 3:\n",
    "                model.args.lr = 1e-3\n",
    "            if epoch >= 6:\n",
    "                model.args.lr = 1e-4\n",
    "            sess.run(model.iterator.make_initializer(train_data))\n",
    "            model.is_training = True\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                while True:\n",
    "                    count += 1\n",
    "                    loss += model.step(sess)\n",
    "                    if count % 300 == 0:\n",
    "                        print('Epoch %d, step %d, with average loss of %.4f in last %d steps;'\n",
    "                              % (epoch, count, loss / (count - last_count), count - last_count))\n",
    "                        loss = 0\n",
    "                        last_count = count\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Epoch %d, finish training \" % epoch + \"took \" +\n",
    "                      time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)) + ';')\n",
    "\n",
    "            ################################## TESTING ################################\n",
    "            sess.run(model.iterator.make_initializer(test_data))\n",
    "            model.is_training = False\n",
    "            start_time = time.time()\n",
    "            HR_s, MRR_s, NDCG_s = [], [], []\n",
    "            predictions_s, labels_s = model.step(sess)\n",
    "\n",
    "            cnt = 1\n",
    "            try:\n",
    "                while True:\n",
    "                    predictions_s, labels_s= model.step(sess)\n",
    "                    hr_s, mrr_s, ndcg_s = evaluate(predictions_s, labels_s)\n",
    "                    HR_s.append(hr_s)\n",
    "                    MRR_s.append(mrr_s)\n",
    "                    NDCG_s.append(ndcg_s)\n",
    "                    cnt += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                hr_s = np.array(HR_s).mean()\n",
    "                mrr_s = np.array(MRR_s).mean()\n",
    "                ndcg_s = np.array(NDCG_s).mean()\n",
    "                hr_s_list.append(hr_s)\n",
    "                mrr_s_list.append(mrr_s)\n",
    "                ndcg_s_list.append(ndcg_s)\n",
    "                print(\"Epoch %d, finish testing \" % epoch + \"took: \" +\n",
    "                      time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)) + ';')\n",
    "                print('Epoch %d, %s HR is %.4f, MRR is %.4f, NDCG is %.4f;' %\n",
    "                      (epoch, name_s, hr_s, mrr_s, ndcg_s))\n",
    "        print('=' * 30 + 'Finish training' + '=' * 30)\n",
    "        print('%s best HR is %.4f, MRR is %.4f, NDCG is %.4f;' %\n",
    "              (name_s, max(hr_s_list), max(mrr_s_list), max(ndcg_s_list)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed107dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
